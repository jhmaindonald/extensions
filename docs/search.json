[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Extensions",
    "section": "",
    "text": "This has notes that enlarge on the contents of the text\n“A Practical Guide to Data Analysis Using R,” by John Maindonald, John Braun and Jeffrey Andrews, Cambridge University Press, 2024."
  },
  {
    "objectID": "boostVSbag.html",
    "href": "boostVSbag.html",
    "title": "Boosting vs Bagging – a Comparison",
    "section": "",
    "text": "The random forest bagging approach takes bootstrap samples from the data, creates a tree for each bootstrap sample (‘bag’), and uses votes across all trees to determine predicted values. Among R packages that implement this approach, note in particular randomForest and ranger. The default number of trees, both for the randomForest function randomForest() and for the ranger function ranger(), is 500. Assuming that the sample data can be treated as a random sample from the population to which results will be applied, the ‘out-of-bag’ error estimate provides an unbiased estimate of the error rate.\nBy contrast, the boosting approach fits one or a small number of trees. For each of the one or more trees, it then calculates the residuals, and fits a tree to the residuals. The process by which each new tree is derived, designed to maximize the ‘gain’, is relatively complex and will not be described here. The parameter eta, with values greater than 0 and at most 1, controls the “learning rate”. It sets a factor by which the contribution of each new tree is scaled when it is added to the current approximation. The default is eta=0.3. Smaller values allow finer control over the learning rate, and provide a way to make the model more robust to overfitting, while slowing computations.\nWhereas the defaults for random forest parameters generally do a good job, and extensive tuning is not required, xgboost parameters do typically require tuning.\nsuppressPackageStartupMessages(library(data.table))\ndata(agaricus.train, package='xgboost')\ntrain &lt;- agaricus.train\ndata(agaricus.test, package='xgboost')\ntest &lt;- agaricus.test"
  },
  {
    "objectID": "boostVSbag.html#a-random-forest-fit-to-the-data-using-ranger-functions",
    "href": "boostVSbag.html#a-random-forest-fit-to-the-data-using-ranger-functions",
    "title": "Boosting vs Bagging – a Comparison",
    "section": "A random forest fit to the data, using ranger functions",
    "text": "A random forest fit to the data, using ranger functions\n\nlibrary(ranger)\nrf1 &lt;- ranger(y=train$label, x=as.matrix(train$data), importance='impurity')\n\n\nimp &lt;- importance(rf1)\nhist(imp)\n\n\n\n\n\n\n\n\nThe importance values are very widely spread. The number of columns (out of 126) that have some level of importance is 116. The 11 rated as having importance equal to zero either have all values the same, or (one column only) have just one value that differs from the rest.\nNow look at the predicted values. Values less than or equal to 0.5 will be treated as implying non-poisonous, with those greater than 0.5 implying poisonous of possibly poisonous:\n\npred &lt;- predict(rf1, data=test$data)$predictions\ntable(pred&gt;0.5, test$label)\n\n       \n          0   1\n  FALSE 835   0\n  TRUE    0 776\n\n\nNow look at the strength of the separation between non-poisonous and poisonous of possibly poisonous mushrooms:\n\nhist(pred, breaks=20)"
  },
  {
    "objectID": "boostVSbag.html#fit-using-xgboost-functions",
    "href": "boostVSbag.html#fit-using-xgboost-functions",
    "title": "Boosting vs Bagging – a Comparison",
    "section": "Fit using xgboost functions",
    "text": "Fit using xgboost functions\nWe first do a simple fit, with no tuning, using the function xgboost(). (For more advanced features, including custom objective and evaluation functions and the facility for checking on performance on test data with each new round, the function xgb.test() will be required.)\n\nlibrary(xgboost)\nbst &lt;- xgboost(data = as.matrix(train$data), label = train$label,  \n               max_depth = 3, eta = 1, nrounds = 2,\n               nthread = 2, objective = \"binary:logistic\")\n\n[1] train-logloss:0.161178 \n[2] train-logloss:0.064728 \n\n## Now calculate a measure of the probability that an\n## observation belongs in the group with label=1, rather\n## than label=0.\npred &lt;- predict(bst, newdata=test$data)\ntable(pred&gt;0.5, test$label)\n\n       \n          0   1\n  FALSE 835   0\n  TRUE    0 776\n\nhist(pred, breaks=20)\n\n\n\n\n\n\n\n\nThe histogram of values of pred indicates that the great majority of observations are very clearly separated into one group rather than the other.\nOr, and preferably, if we use xgb.train(), we can do\n\ndtrain &lt;- xgb.DMatrix(train$data, label = train$label, nthread = 2)\ndtest &lt;- xgb.DMatrix(test$data, label = test$label, nthread = 2)\nwatchlist &lt;- list(eval = dtest, train = dtrain)\nparam &lt;- list(max_depth = 3, eta = 0.75, nthread = 2)\nbst &lt;- xgb.train(param, dtrain, nrounds = 4, watchlist, \n                 objective = \"binary:logistic\")\n\n[1] eval-logloss:0.232339   train-logloss:0.230181 \n[2] eval-logloss:0.110409   train-logloss:0.113869 \n[3] eval-logloss:0.054348   train-logloss:0.055521 \n[4] eval-logloss:0.029359   train-logloss:0.030794 \n\n\nWhat is unexpected here is that the root mean square error is, from the second round on, lower on the test data than on the training data, with the difference increasing with each successive round. This makes it doubtful whether the test data was genuinely a random sample from the total data. We therefore do a new random split of the total data into training and test subsets. Before proceeding, we will check for columns in the data that are constant. These are, in order to reduce the computational load, best removed:\n\nlab &lt;- c(train$label, test$label)\ndat &lt;- rbind(train$data,test$data)\n(rmcol &lt;- (1:ncol(dat))[apply(dat, 2, function(x)length(unique(x))==1)])\n\n [1]  33  35  38  57  59  88  89  97 103 104\n\ndat &lt;- dat[, -rmcol]\n\nNow create a new split into the training and test data, and hence new xgb.DMatrix objects.\n\nset.seed(67)\ntestrows &lt;- sample(1:nrow(dat), size=nrow(test$data))\nDtrain &lt;- xgb.DMatrix(dat[-testrows, ], label = lab[-testrows], \n                      nthread = 2)\nDtest &lt;- xgb.DMatrix(dat[testrows,], label = lab[testrows], \n                      nthread = 2)\nwatchlist &lt;- list(eval = Dtest, train = Dtrain)\n\n\nparam &lt;- list(max_depth = 3, eta = 0.75, nthread = 2)\nbst &lt;- xgb.train(param, Dtrain, nrounds = 60, watchlist, \n                 print_every_n = 3, objective = \"binary:logistic\")\n\n[1] eval-logloss:0.230585   train-logloss:0.230804 \n[4] eval-logloss:0.030782   train-logloss:0.026497 \n[7] eval-logloss:0.008330   train-logloss:0.005590 \n[10]    eval-logloss:0.003945   train-logloss:0.002763 \n[13]    eval-logloss:0.001929   train-logloss:0.001375 \n[16]    eval-logloss:0.001350   train-logloss:0.000942 \n[19]    eval-logloss:0.001020   train-logloss:0.000759 \n[22]    eval-logloss:0.000907   train-logloss:0.000667 \n[25]    eval-logloss:0.000814   train-logloss:0.000615 \n[28]    eval-logloss:0.000791   train-logloss:0.000563 \n[31]    eval-logloss:0.000751   train-logloss:0.000539 \n[34]    eval-logloss:0.000745   train-logloss:0.000530 \n[37]    eval-logloss:0.000726   train-logloss:0.000522 \n[40]    eval-logloss:0.000729   train-logloss:0.000515 \n[43]    eval-logloss:0.000717   train-logloss:0.000508 \n[46]    eval-logloss:0.000699   train-logloss:0.000503 \n[49]    eval-logloss:0.000697   train-logloss:0.000498 \n[52]    eval-logloss:0.000693   train-logloss:0.000493 \n[55]    eval-logloss:0.000686   train-logloss:0.000490 \n[58]    eval-logloss:0.000678   train-logloss:0.000486 \n[60]    eval-logloss:0.000679   train-logloss:0.000484 \n\n\nThus, around 58 rounds appear required, in order to minimize logloss. For purposes of distinguishing between the two classes of mushrooms, this is gross overkill. Just one round is enough to give a very clear separation. Try:\n\nbst1 &lt;- xgboost(Dtrain, nrounds = 1, eta=.75, \n                 objective = \"binary:logistic\")\n\n[1] train-logloss:0.204290 \n\nhist(predict(bst1, newdata=Dtest))\n\n\n\n\n\n\n\n\nNow look at importance measures (1) from the single round fit (bst1), and (2) from the 64 rounds fit (bst):\n\nimbst1 &lt;- xgb.importance(model=bst1)\nimbst &lt;- xgb.importance(model=bst)\n\"Importances identified\"\n\n[1] \"Importances identified\"\n\nc(\"Simgle round fit\"= dim(imbst1)[1], \"64 round fit\"= dim(imbst)[1])\n\nSimgle round fit     64 round fit \n               9               35 \n\n\nThe following plots the largest 10 importance values (the column is labeled Gain in the output) from the 64 round fit:\n\nxgb.plot.importance(imbst, top_n=10)\n\n\n\n\n\n\n\n\nJust 35 of the 116 columns have been used, with most giving only a very slight gain. Consider carefully what this means. The implication is that after accounting for effects that can be accounted for using these 35 columns, other columns add nothing extra. This happens because of the correlation structure. The first tree that is chosen sets the scene for what follows. The variety of trees that are chosen by ranger() gives an indication of how different that initial tree might be. Each new bootstrap sample simulates the taking of a new random sample from the population from which the original sample was taken.\nBy contrast, ranger() gives some level of importance to all features:\n\nlibrary(ranger)\nbag &lt;- ranger(y=lab, x=dat, importance='impurity')\nimbag &lt;- importance(bag)\nlength(imbag)\n\n[1] 116\n\nsummary(imbag)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  0.000   1.453   3.818  17.366  16.181 233.530 \n\n\nLook also at ranger predictions:\n\npred &lt;- predict(bag, data=test$data)$predictions\ntable(pred&gt;0.5, test$label)\n\n       \n          0   1\n  FALSE 835   0\n  TRUE    0 776\n\nhist(pred)\n\n\n\n\n\n\n\n\nNotice the very clear separation between values that round to 0 (not poisonous) and 1 (poisonous or possibly poisonous).\nWhat happens if we remove all the columns that were not given any level of importance in the xgboost.train() analysis, and then fit a random forest?\n\nrnam &lt;- unlist(imbst[,1])\ndatbst &lt;- dat[, rnam]\nrfSome &lt;- ranger(y=lab[-testrows], x=datbst[-testrows, ], importance='impurity')\npred &lt;- predict(rfSome, data=dat[testrows,])$predictions\ntable(pred&gt;0.5, lab[testrows])\n\n       \n          0   1\n  FALSE 840   0\n  TRUE    0 771\n\nhist(pred, breaks=20)"
  },
  {
    "objectID": "boostVSbag.html#a-more-conventional-tree-fit-using-rpartrpart",
    "href": "boostVSbag.html#a-more-conventional-tree-fit-using-rpartrpart",
    "title": "Boosting vs Bagging – a Comparison",
    "section": "A more conventional tree – fit using rpart::rpart",
    "text": "A more conventional tree – fit using rpart::rpart\n\nlibrary(rpart)\ndatt &lt;- cbind(label=lab, as.data.frame(as.matrix(dat)))\nrp &lt;- rpart(label~., data=datt, method=\"class\", cp=0.001)\npr &lt;- predict(rp, type='vector')\ntable(pr, datt$label)\n\n   \npr     0    1\n  1 4208    0\n  2    0 3916"
  },
  {
    "objectID": "boostVSbag.html#the-diamonds-dataset-this-is-a-more-serious-challenge",
    "href": "boostVSbag.html#the-diamonds-dataset-this-is-a-more-serious-challenge",
    "title": "Boosting vs Bagging – a Comparison",
    "section": "The diamonds dataset – this is a more serious challenge",
    "text": "The diamonds dataset – this is a more serious challenge\nFor the agaricus dataset, distinguishing the two classes of mushroom was an easy task – all three methods that were tried did an effective job. For a more realistic comparison of the methodologies, we will use the gplot2::diamonds dataset.\nThe website https://lorentzen.ch/index.php/2021/04/16/a-curious-fact-on-the-diamonds-dataset/ (Michael Mayer) points out that that more than 25% of the observations appear to be duplicates. For example, there are exactly six diamonds of 2.01 carat and a price of 16,778 USD that all have the same color, cut and clarity, with other measures showing different perspectives on the same data. Thus observe:\n\ndiamonds &lt;- ggplot2::diamonds\nid &lt;- apply(diamonds[,c(1:4,7)], 1, paste0, collapse='-')\nkeepFirst &lt;- !duplicated(id) ## all except the first\n## keepLast &lt;- rev(!duplicated(rev(id)))\ndiamondA &lt;- diamonds[keepFirst, ]       ## Retain only the first \nc(nrow(diamondA),nrow(diamondA)/4)      ## 39756, 9939\n\n[1] 39756  9939\n\n## diamondZ &lt;- diamonds[keepLast, ]     ## Retain only the last \ntable(keepFirst)/length(id)\n\nkeepFirst\n    FALSE      TRUE \n0.2629588 0.7370412"
  },
  {
    "objectID": "boostVSbag.html#keepfirst",
    "href": "boostVSbag.html#keepfirst",
    "title": "Boosting vs Bagging – a Comparison",
    "section": "keepFirst",
    "text": "keepFirst"
  },
  {
    "objectID": "boostVSbag.html#false-true",
    "href": "boostVSbag.html#false-true",
    "title": "Boosting vs Bagging – a Comparison",
    "section": "FALSE TRUE",
    "text": "FALSE TRUE"
  },
  {
    "objectID": "boostVSbag.html#section",
    "href": "boostVSbag.html#section",
    "title": "Boosting vs Bagging – a Comparison",
    "section": "0.2629588 0.7370412",
    "text": "0.2629588 0.7370412\nThe ranger package is an alternative to randomForest that is much more efficient for working with large datasets. Working with the dataset that retains only the first of the ‘duplicates’, one finds:\n\nset.seed(31)\nlibrary(ranger)\nY &lt;- diamondA[,\"price\", drop=T]\nsamp50pc &lt;- sample(1:nrow(diamondA), size=9939*2)\n(diamond50pc.rf &lt;- ranger(x=diamondA[samp50pc,-7], y=log(Y[samp50pc])))\n\nRanger result\n\nCall:\n ranger(x = diamondA[samp50pc, -7], y = log(Y[samp50pc])) \n\nType:                             Regression \nNumber of trees:                  500 \nSample size:                      19878 \nNumber of independent variables:  9 \nMtry:                             3 \nTarget node size:                 5 \nVariable importance mode:         none \nSplitrule:                        variance \nOOB prediction error (MSE):       0.0107198 \nR squared (OOB):                  0.9886767 \n\n## OOB prediction error (MSE):       0.0107198 \n## OOB prediction error (MSE):       0.01072289  ## Repeat  calculation\n\n\npred &lt;- predict(diamond50pc.rf,\n                data=diamondA[-samp50pc,-7])$predictions\nsum((pred-log(Y[-samp50pc]))^2)/length(pred)\n\n[1] 0.01141683\n\n\nAs expected this is very similar to the OOB mse.\n\nFit using xgboost::xgb.train()\nThe diamonds data includes some columns that are factors or ordered factors.\n\ndiamondA[1,]\n\n# A tibble: 1 × 10\n  carat cut   color clarity depth table price     x     y     z\n  &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  0.23 Ideal E     SI2      61.5    55   326  3.95  3.98  2.43\n\n\nObserve that color is an ordered factor – there is an order of preference from D (best) to J (worst). The xgboost functions xgboost() and xgb.DMatrix() require a model matrix as input, rather than a dataframe that can include factors and ordered factors in its columns. The function sparse.model.matrix() from the Matrix package can be used to create the needed model matrix. The function xgb.DMatrix() goes on to create an xgb.DMatrix object of the type needed for use of the function xgb.train().\n\nlibrary(Matrix)\nsparsem &lt;- sparse.model.matrix(price~., data=diamondA)[,-1]\n\nSpecifying price as the dependent variable ensures that the corresponding is excluded from the matrix that is created. Also, the initial column of 1’s serves no useful purpose for the tree-based calculations, and is removed.\n\nDtrain &lt;- xgb.DMatrix(as.matrix(sparsem[samp50pc, ]), \n                      label = log(Y[samp50pc]), nthread = 2)\nDtest &lt;- xgb.DMatrix(sparsem[-samp50pc,], \n                     label = log(Y[-samp50pc]), nthread = 2)\nwatchlist &lt;- list(eval = Dtest, train = Dtrain)\nparam &lt;- list(max_depth = 5, eta = 0.4, nthread = 2)\nbst &lt;- xgb.train(param, Dtrain, nrounds = 81, watchlist, \n                 print_every_n = 3)\n\n[1] eval-rmse:4.560709  train-rmse:4.563817 \n[4] eval-rmse:1.001089  train-rmse:1.000100 \n[7] eval-rmse:0.252409  train-rmse:0.248454 \n[10]    eval-rmse:0.132035  train-rmse:0.123515 \n[13]    eval-rmse:0.118572  train-rmse:0.108683 \n[16]    eval-rmse:0.115815  train-rmse:0.105641 \n[19]    eval-rmse:0.114015  train-rmse:0.102737 \n[22]    eval-rmse:0.112247  train-rmse:0.100619 \n[25]    eval-rmse:0.110841  train-rmse:0.098610 \n[28]    eval-rmse:0.109701  train-rmse:0.096937 \n[31]    eval-rmse:0.109257  train-rmse:0.096040 \n[34]    eval-rmse:0.108454  train-rmse:0.094530 \n[37]    eval-rmse:0.107567  train-rmse:0.093120 \n[40]    eval-rmse:0.106824  train-rmse:0.091683 \n[43]    eval-rmse:0.106709  train-rmse:0.091043 \n[46]    eval-rmse:0.105924  train-rmse:0.089707 \n[49]    eval-rmse:0.105779  train-rmse:0.089122 \n[52]    eval-rmse:0.105620  train-rmse:0.088774 \n[55]    eval-rmse:0.105428  train-rmse:0.088140 \n[58]    eval-rmse:0.104845  train-rmse:0.086958 \n[61]    eval-rmse:0.104801  train-rmse:0.086337 \n[64]    eval-rmse:0.104729  train-rmse:0.085900 \n[67]    eval-rmse:0.104302  train-rmse:0.084950 \n[70]    eval-rmse:0.104085  train-rmse:0.084115 \n[73]    eval-rmse:0.104079  train-rmse:0.083857 \n[76]    eval-rmse:0.103845  train-rmse:0.083048 \n[79]    eval-rmse:0.103723  train-rmse:0.082711 \n[81]    eval-rmse:0.103738  train-rmse:0.082495"
  }
]